# DataGenLLM

DataGenLLM is a project that focuses on the automated generation of text data for classification problems using Large Language Models (LLMs). This project demonstrates the ability of LLMs to produce diverse and accurate datasets that can be utilized to fine-tune smaller models like BERT for specific classification tasks.

For a detailed explanation of the project goals, design, and results, refer to our ðŸ“„ [DataGenLLM Project Report](https://drive.google.com/file/d/160ERnPz1vyM2OiLii8MmPNyUBtxSpCo3/view?usp=share_link).

## Repository Structure

Below is the file structure of the DataGenLLM repository, outlining the main components and their purpose:

- `Trained_Models/`: This directory contains the models that have been fine-tuned on the generated datasets.  
- `Oracle_Datasets/`: The original datasets used as a gold standard for training and evaluating models.
- `Generated_Datasets/`: The datasets generated by the LLM, ready for use in further model training and evaluation.
- `metrics/`: Scripts for calculating Self-BLEU.
- `DatasetValidation.ipynb`: Jupyter notebook detailing the data validation process for the generated datasets.
- `DatasetGeneration.ipynb`: Jupyter notebook demonstrating the process of generating datasets using LLMs.
- `BertFineTuning.ipynb`: Jupyter notebook illustrating the fine-tuning process of BERT models on the generated datasets.
- `BertClassifier.py`: Python script containing the BERT classification model's implementation.

## Getting Started

To use this repository:

1. Clone the repo to your local machine using `git clone`.
2. Download and unzip the folders from [Generated_Datasets.zip](https://drive.google.com/file/d/1P0Rkwo2ic_HTpjHl8SyurK1Tvu1G6SY9/view?usp=drive_link), [Oracle_Datasets.zip](https://drive.google.com/file/d/1H1hDpV6-jtfPK14fcTlxdkS2NU0mxf8V/view?usp=drive_link), and [Trained_Models.zip](https://drive.google.com/file/d/1trY_UIVNFnLehuOoYrunvMH8qYG_QERU/view?usp=drive_link). Place these folders at the root of the repository. 
3. The easiest way to run the code is to upload all the repo info into google drive and run the 3 scripts using the T4 GPU. When running the scripts, the repopath variable needs to be modified to refer to the root of the repository. 
4. Alternatively, install required dependencies listed in `requirements.txt` and explore the Jupyter notebooks to understand the dataset generation, validation, and model fine-tuning processes.  When running the scripts, the repopath variable needs to be modified to refer to the root of the repository. 
	1. Run `DatasetGeneration.ipynb` to generate data.
	2. Run `DatasetValidation.ipynb` to do validation on generated data.
	3. Run `BertFineTuning.ipynb` to evaluate the generated data.

<!--## Contribution

Contributions to DataGenLLM are welcome. Please read `CONTRIBUTING.md` for details on our code of conduct, and the process for submitting pull requests.
-->
<!--## License

This project is licensed under the MIT License - see the `LICENSE.md` file for details.
-->
## Acknowledgments

- Chung, Kamar, and Amershi for their foundational work on LLM-based text data generation.
- The Hugging Face community for their resources on transformer models.
