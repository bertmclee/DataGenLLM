# DataGenLLM

DataGenLLM is a project that focuses on the automated generation of text data for classification problems using Large Language Models (LLMs). This project demonstrates the ability of LLMs to produce diverse and accurate datasets that can be utilized to fine-tune smaller models like BERT for specific classification tasks.

## Repository Structure

Below is the file structure of the DataGenLLM repository, outlining the main components and their purpose:

- `Trained_Models/`: This directory contains the models that have been fine-tuned on the generated datasets.  
- `Oracle_Datasets/`: The original datasets used as a gold standard for training and evaluating models.
- `Generated_Datasets/`: The datasets generated by the LLM, ready for use in further model training and evaluation.
- `metrics/`: Scripts or modules for calculating metrics such as accuracy, precision, recall, F1-score, and Self-BLEU.
- `DatasetValidation.ipynb`: Jupyter notebook detailing the data validation process for the generated datasets.
- `DatasetGeneration.ipynb`: Jupyter notebook demonstrating the process of generating datasets using LLMs.
- `BertFineTuning.ipynb`: Jupyter notebook illustrating the fine-tuning process of BERT models on the generated datasets.
- `BertClassifier.py`: Python script containing the BERT classification model's implementation.

## Getting Started

To use this repository:

1. Clone the repo to your local machine using `git clone`.
<!--2. Install required dependencies listed in `requirements.txt`.-->
2. Explore the Jupyter notebooks to understand the dataset generation, validation, and model fine-tuning processes.
	1. Run `DatasetGeneration.ipynb` to generate data.
	2. Run `DatasetValidation.ipynb` to do validation on generated data.
	3. Run `BertFineTuning.ipynb` to evaluate the generated data.

<!--## Contribution

Contributions to DataGenLLM are welcome. Please read `CONTRIBUTING.md` for details on our code of conduct, and the process for submitting pull requests.
-->
<!--## License

This project is licensed under the MIT License - see the `LICENSE.md` file for details.
-->
## Acknowledgments

- Chung, Kamar, and Amershi for their foundational work on LLM-based text data generation.
- The Hugging Face community for their resources on transformer models.
